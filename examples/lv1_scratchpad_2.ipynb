{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8504f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population Size: 10\n",
      "\n",
      "--- First Individual ---\n",
      "Row  | Expression                     | Raw\n",
      "--------------------------------------------------\n",
      "0    | v_0 = SIN(x_0, x_1)            | [0 1]\n",
      "1    | v_1 = SUB(x_0, x_0)            | [0 0]\n",
      "2    | v_2 = SUB(v_1, v_1)            | [3 3]\n",
      "3    | v_3 = SIN(v_2, v_0)            | [4 2]\n",
      "4    | v_4 = SIN(x_1, x_1)            | [1 1]\n",
      "\n",
      "Subset Size: 3\n",
      "\n",
      "Distance Matrix Shape: (10, 10)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import struct\n",
    "import chex\n",
    "from typing import Any, Type, TypeVar, Generic, ClassVar, Union, Iterator, List, Optional, Tuple\n",
    "\n",
    "# --- Types & Constants ---\n",
    "G = TypeVar(\"G\", bound=\"BaseGenome\")\n",
    "\n",
    "class DistanceMetric:\n",
    "    HAMMING = \"hamming\"\n",
    "    EUCLIDEAN = \"euclidean\"\n",
    "    MANHATTAN = \"manhattan\"\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class LinearGenomeConfig:\n",
    "    length: int           # L: Number of instructions\n",
    "    num_inputs: int       # N: Number of input features\n",
    "    num_ops: int          # Number of available functions\n",
    "    max_arity: int = 2    # Arguments per instruction\n",
    "\n",
    "# ==========================================\n",
    "# 2. BASE GENOME (Abstract Individual)\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class BaseGenome:\n",
    "    \"\"\"\n",
    "    Abstract base class for a Single Individual.\n",
    "    \"\"\"\n",
    "    # --- Abstract Interface ---\n",
    "    @classmethod\n",
    "    def random_init(cls: Type[G], key: chex.PRNGKey, config: Any) -> G:\n",
    "        \"\"\"Create ONE random genome.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def distance(self, other: \"BaseGenome\", metric: str) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def autocorrect(self, config: Any) -> \"BaseGenome\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # --- Shared Logic (Vectorization) ---\n",
    "    @classmethod\n",
    "    def create_population(cls: Type[G], key: chex.PRNGKey, config: Any, pop_size: int) -> G:\n",
    "        \"\"\"\n",
    "        Vectorizes random_init to create a raw batch of genomes.\n",
    "        Returns object with shape (Pop, ...).\n",
    "        \"\"\"\n",
    "        keys = jax.random.split(key, pop_size)\n",
    "        return jax.vmap(cls.random_init, in_axes=(0, None))(keys, config)\n",
    "\n",
    "# ==========================================\n",
    "# 3. CONCRETE GENOME (Linear Implementation)\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class LinearGenome(BaseGenome):\n",
    "    ops: chex.Array   # Shape (L,)\n",
    "    args: chex.Array  # Shape (L, Arity)\n",
    "\n",
    "    @classmethod\n",
    "    def random_init(cls, key: chex.PRNGKey, config: LinearGenomeConfig) -> \"LinearGenome\":\n",
    "        k_ops, k_args = jax.random.split(key)\n",
    "        \n",
    "        # 1. Random Opcodes\n",
    "        ops = jax.random.randint(k_ops, (config.length,), 0, config.num_ops)\n",
    "        \n",
    "        # 2. Topological Arguments (Row 'i' can only see 0..N+i-1)\n",
    "        row_limits = jnp.arange(config.num_inputs, config.num_inputs + config.length)\n",
    "        \n",
    "        def gen_row(rk, climit):\n",
    "            return jax.random.randint(rk, (config.max_arity,), 0, climit)\n",
    "            \n",
    "        row_keys = jax.random.split(k_args, config.length)\n",
    "        args = jax.vmap(gen_row)(row_keys, row_limits)\n",
    "        \n",
    "        return cls(ops=ops, args=args)\n",
    "\n",
    "    def autocorrect(self, config: LinearGenomeConfig) -> \"LinearGenome\":\n",
    "        \"\"\"Fixes invalid references to ensure Topological DAG structure.\"\"\"\n",
    "        valid_ops = jnp.clip(self.ops, 0, config.num_ops - 1)\n",
    "        \n",
    "        # Re-calculate limits\n",
    "        row_limits = jnp.arange(config.num_inputs, config.num_inputs + config.length)\n",
    "        # Max valid index is limit - 1\n",
    "        max_indices = row_limits[:, None] - 1\n",
    "        valid_args = jnp.clip(self.args, 0, max_indices)\n",
    "        \n",
    "        return self.replace(ops=valid_ops, args=valid_args)\n",
    "\n",
    "    def distance(self, other: \"LinearGenome\", metric: str = \"hamming\") -> float:\n",
    "        d_ops = jnp.sum(self.ops != other.ops)\n",
    "        d_args = jnp.sum(self.args != other.args)\n",
    "        return (d_ops + d_args).astype(jnp.float32)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return self.ops.shape[-1]\n",
    "\n",
    "    def render(self, config: LinearGenomeConfig, op_names: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Human-Readable String Representation.\"\"\"\n",
    "        ops_cpu = np.array(self.ops)\n",
    "        args_cpu = np.array(self.args)\n",
    "        lines = [f\"{'Row':<4} | {'Expression':<30} | {'Raw'}\"]\n",
    "        lines.append(\"-\" * 50)\n",
    "        \n",
    "        for i in range(config.length):\n",
    "            op_idx = int(ops_cpu[i])\n",
    "            op_str = op_names[op_idx] if op_names and op_idx < len(op_names) else f\"OP_{op_idx}\"\n",
    "            \n",
    "            decoded_args = []\n",
    "            for arg_idx in args_cpu[i]:\n",
    "                if arg_idx < config.num_inputs:\n",
    "                    decoded_args.append(f\"x_{arg_idx}\")\n",
    "                else:\n",
    "                    decoded_args.append(f\"v_{arg_idx - config.num_inputs}\")\n",
    "            \n",
    "            expr = f\"v_{i} = {op_str}({', '.join(decoded_args)})\"\n",
    "            lines.append(f\"{i:<4} | {expr:<30} | {args_cpu[i]}\")\n",
    "            \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<LinearGenome(L={self.ops.shape[-1]})>\"\n",
    "\n",
    "# ==========================================\n",
    "# 4. BASE POPULATION (Abstract Container)\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class BasePopulation(Generic[G]):\n",
    "    \"\"\"\n",
    "    Abstract Container. \n",
    "    Implements List-like behavior and automated Vectorization.\n",
    "    \"\"\"\n",
    "    genes: G\n",
    "    fitness: chex.Array\n",
    "    \n",
    "    GENOME_CLS: ClassVar[Type[G]]\n",
    "\n",
    "    # --- The \"Kebab\" Interface (List Behavior) ---\n",
    "    def __len__(self) -> int:\n",
    "        return int(self.fitness.shape[0])\n",
    "\n",
    "    def __getitem__(self, key: Union[int, slice, chex.Array]) -> Union[G, \"BasePopulation[G]\"]:\n",
    "        # Slice the genes Pytree automatically\n",
    "        sliced_genes = jax.tree_util.tree_map(lambda x: x[key], self.genes)\n",
    "        \n",
    "        if isinstance(key, int):\n",
    "            # Return Single Genome\n",
    "            return sliced_genes\n",
    "        else:\n",
    "            # Return Sliced Population\n",
    "            return self.replace(genes=sliced_genes, fitness=self.fitness[key])\n",
    "\n",
    "    def __iter__(self) -> Iterator[G]:\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "\n",
    "    # --- Automated Logic (Vectorized) ---\n",
    "    def autocorrect(self, config: Any) -> \"BasePopulation[G]\":\n",
    "        \"\"\"Autocorrects entire batch.\"\"\"\n",
    "        new_genes = jax.vmap(lambda g: g.autocorrect(config))(self.genes)\n",
    "        return self.replace(genes=new_genes)\n",
    "\n",
    "    def distance_matrix(self, metric: str = \"hamming\") -> chex.Array:\n",
    "        \"\"\"Computes N x N distance matrix.\"\"\"\n",
    "        pair_fn = lambda g1, g2: g1.distance(g2, metric)\n",
    "        return jax.vmap(jax.vmap(pair_fn, in_axes=(None, 0)), in_axes=(0, None))(self.genes, self.genes)\n",
    "\n",
    "# ==========================================\n",
    "# 5. CONCRETE POPULATION (Linear Implementation)\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class LinearPopulation(BasePopulation[LinearGenome]):\n",
    "    genes: LinearGenome\n",
    "    fitness: chex.Array\n",
    "    \n",
    "    GENOME_CLS: ClassVar[Type[LinearGenome]] = LinearGenome\n",
    "\n",
    "    @classmethod\n",
    "    def init_random(cls, key: chex.PRNGKey, config: LinearGenomeConfig, size: int) -> \"LinearPopulation\":\n",
    "        # 1. Use Genome Factory (Returns Batch)\n",
    "        batched_genes = LinearGenome.create_population(key, config, size)\n",
    "        # 2. Init Fitness\n",
    "        initial_fitness = jnp.full((size,), -jnp.inf)\n",
    "        return cls(genes=batched_genes, fitness=initial_fitness)\n",
    "    \n",
    "    \n",
    "# 1. Config\n",
    "config = LinearGenomeConfig(length=5, num_inputs=2, num_ops=4)\n",
    "key = jax.random.PRNGKey(42)\n",
    "op_names = [\"ADD\", \"SUB\", \"MUL\", \"SIN\"]\n",
    "\n",
    "# 2. Create Population (The Easy Way)\n",
    "pop = LinearPopulation.init_random(key, config, size=10)\n",
    "\n",
    "# 3. Test \"Kebab\" Indexing\n",
    "print(f\"Population Size: {len(pop)}\")\n",
    "\n",
    "first_guy = pop[0] # Returns LinearGenome\n",
    "print(\"\\n--- First Individual ---\")\n",
    "print(first_guy.render(config, op_names))\n",
    "\n",
    "# 4. Test Slicing\n",
    "subset = pop[:3] # Returns LinearPopulation\n",
    "print(f\"\\nSubset Size: {len(subset)}\")\n",
    "\n",
    "# 5. Test Automated Vectorization\n",
    "matrix = pop.distance_matrix()\n",
    "print(f\"\\nDistance Matrix Shape: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed2c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import chex\n",
    "from functools import partial\n",
    "from typing import Any, Tuple, Callable, TypeVar, Generic\n",
    "\n",
    "# --- A. Define Robust Primitives (The Instruction Set) ---\n",
    "def op_add(x, y): return x + y\n",
    "def op_sub(x, y): return x - y\n",
    "def op_mul(x, y): return x * y\n",
    "def op_div(x, y): return jnp.where(jnp.abs(y) < 0.001, 1.0, x / y) # Protected\n",
    "def op_sin(x, y): return jnp.sin(x)\n",
    "def op_cos(x, y): return jnp.cos(x)\n",
    "\n",
    "# The registry used by lax.switch (Index matches OpCode)\n",
    "OP_FUNCTIONS = (op_add, op_sub, op_mul, op_div, op_sin, op_cos)\n",
    "OP_NAMES = [\"ADD\", \"SUB\", \"MUL\", \"DIV\", \"SIN\", \"COS\"]\n",
    "\n",
    "# --- B. The Abstract Evaluator ---\n",
    "G = TypeVar(\"G\", bound=\"BaseGenome\")\n",
    "C = TypeVar(\"C\") # Config Type\n",
    "D = TypeVar(\"D\") # Data Type\n",
    "\n",
    "@struct.dataclass\n",
    "class BaseEvaluator(Generic[G, C, D]):\n",
    "    config: C\n",
    "\n",
    "    def evaluate(self, genome: G, data: D) -> float:\n",
    "        \"\"\"Abstract: Returns scalar fitness for one genome.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate_population(self, population: \"BasePopulation[G]\", data: D) -> \"BasePopulation[G]\":\n",
    "        \"\"\"Auto-Vectorization: Pop x Data -> Pop (with Fitness)\"\"\"\n",
    "        # vmap over genes (axis 0), keep self/data constant\n",
    "        fitness_scores = jax.vmap(self.evaluate, in_axes=(0, None))(population.genes, data)\n",
    "        return population.replace(fitness=fitness_scores)\n",
    "\n",
    "# --- C. The Concrete Linear Evaluator ---\n",
    "# Data Type: Tuple(Inputs, Targets)\n",
    "RegressionData = Tuple[chex.Array, chex.Array]\n",
    "\n",
    "@struct.dataclass\n",
    "class LinearGPEvaluator(BaseEvaluator[LinearGenome, LinearGenomeConfig, RegressionData]):\n",
    "    \n",
    "    def predict_one(self, genome: LinearGenome, x_input: chex.Array) -> chex.Array:\n",
    "        \"\"\"\n",
    "        Interprets one genome on one input vector.\n",
    "        Returns: Shape (L,) -> The output of EVERY instruction row.\n",
    "        \"\"\"\n",
    "        # 1. Initialize Memory\n",
    "        total_mem = self.config.num_inputs + self.config.length\n",
    "        memory = jnp.zeros(total_mem)\n",
    "        memory = memory.at[:self.config.num_inputs].set(x_input)\n",
    "        \n",
    "        # 2. Execution Loop\n",
    "        def step(current_mem, inputs):\n",
    "            mem, write_idx = current_mem\n",
    "            op_code, arg_indices = inputs\n",
    "            \n",
    "            # Fetch & Execute\n",
    "            args_val = jnp.take(mem, arg_indices)\n",
    "            res = jax.lax.switch(op_code, OP_FUNCTIONS, args_val[0], args_val[1])\n",
    "            res = jnp.nan_to_num(res, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            \n",
    "            # Write to register\n",
    "            new_mem = mem.at[write_idx].set(res)\n",
    "            \n",
    "            # CRITICAL CHANGE: \n",
    "            # We return 'res' as the history item, NOT 'new_mem'.\n",
    "            # 'res' is the output of the current Atomic Tree.\n",
    "            return (new_mem, write_idx + 1), res\n",
    "\n",
    "        init_val = (memory, self.config.num_inputs)\n",
    "        \n",
    "        # history will have shape (Length,)\n",
    "        # It contains [Output_Row_0, Output_Row_1, ..., Output_Row_L]\n",
    "        (_, _), history = jax.lax.scan(step, init_val, (genome.ops, genome.args))\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def evaluate(self, genome: LinearGenome, data: RegressionData) -> float:\n",
    "        X, Y = data\n",
    "        \n",
    "        # 1. Vectorize Prediction (Data Parallelism)\n",
    "        # predict_one returns shape (Length,)\n",
    "        # vmap(predict_one) over X returns shape (Batch_Size, Length)\n",
    "        all_preds = jax.vmap(self.predict_one, in_axes=(None, 0))(genome, X)\n",
    "        \n",
    "        # all_preds shape: (1000 samples, 20 instructions)\n",
    "        # Y shape:         (1000 samples,)\n",
    "        \n",
    "        # 2. Calculate MSE for EVERY instruction column\n",
    "        # We broadcast Y to match shape (1000, 20)\n",
    "        Y_bcast = Y[:, None]\n",
    "        \n",
    "        # Squared Error: (1000, 20)\n",
    "        squared_errors = (all_preds - Y_bcast) ** 2\n",
    "        \n",
    "        # Mean over Data Axis (0) -> Result shape (20,)\n",
    "        # This gives us the MSE for Atomic Tree 0, Atomic Tree 1...\n",
    "        mse_per_tree = jnp.mean(squared_errors, axis=0)\n",
    "        \n",
    "        # 3. The \"Symbiotic\" Selection\n",
    "        # The fitness of the Genome is the fitness of its BEST Atomic Tree.\n",
    "        # We minimize MSE, so we take the minimum error found.\n",
    "        best_mse = jnp.min(mse_per_tree)\n",
    "        \n",
    "        return -mse_per_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e7e3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: X=(1000, 5), y=(1000,)\n",
      "Initializing Population of 2000 individuals...\n",
      "Compiling & Evaluating...\n",
      "\n",
      "Optimization Complete!\n",
      "Best MSE: 3451.2764\n",
      "------------------------------\n",
      "Best Program Structure:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. PREPARE DATA (Sklearn)\n",
    "# Generate a simple regression problem: y = combination of 5 features\n",
    "X_raw, y_raw = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "# Important: Scale data for Neural/Evolutionary methods\n",
    "scaler_x = StandardScaler()\n",
    "X_scaled = scaler_x.fit_transform(X_raw)\n",
    "# JAX expects arrays, not numpy\n",
    "X_jax = jnp.array(X_scaled)\n",
    "y_jax = jnp.array(y_raw)\n",
    "\n",
    "print(f\"Dataset Shape: X={X_jax.shape}, y={y_jax.shape}\")\n",
    "\n",
    "# 2. CONFIGURE ENGINE\n",
    "# We have 5 inputs (features)\n",
    "# We give it 20 instructions (depth) to solve the problem\n",
    "config = LinearGenomeConfig(\n",
    "    length=20, \n",
    "    num_inputs=5, \n",
    "    num_ops=len(OP_FUNCTIONS), # 6 operators\n",
    "    max_arity=2\n",
    ")\n",
    "\n",
    "# 3. INITIALIZE POPULATION\n",
    "# Let's create a massive batch to see JAX speed\n",
    "POP_SIZE = 2000\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "print(f\"Initializing Population of {POP_SIZE} individuals...\")\n",
    "# Calls the Factory we wrote earlier\n",
    "population = LinearPopulation.init_random(key, config, size=POP_SIZE)\n",
    "\n",
    "# 4. RUN EVALUATION\n",
    "print(\"Compiling & Evaluating...\")\n",
    "evaluator = LinearGPEvaluator(config=config)\n",
    "\n",
    "# This triggers the JIT compilation (might take 1-2 sec first time)\n",
    "# Then it runs 2000 genomes * 1000 samples = 2 Million evaluations instantly\n",
    "evaluated_pop = evaluator.evaluate_population(population, (X_jax, y_jax))\n",
    "\n",
    "# 5. INSPECT RESULTS\n",
    "# Get the winner\n",
    "best_fitness = jnp.max(evaluated_pop.fitness)\n",
    "\n",
    "print(f\"\\nOptimization Complete!\")\n",
    "print(f\"Best MSE: {-best_fitness:.4f}\") # Remember we minimized negative MSE\n",
    "print(\"-\" * 30)\n",
    "print(\"Best Program Structure:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0860265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row  | Expression                     | Raw\n",
      "--------------------------------------------------\n",
      "0    | v_0 = COS(x_2, x_4)            | [2 4]\n",
      "1    | v_1 = DIV(x_1, v_0)            | [1 5]\n",
      "2    | v_2 = MUL(v_1, v_0)            | [6 5]\n",
      "3    | v_3 = SUB(v_1, x_0)            | [6 0]\n",
      "4    | v_4 = COS(x_3, v_0)            | [3 5]\n",
      "5    | v_5 = COS(x_2, v_1)            | [2 6]\n",
      "6    | v_6 = MUL(v_0, x_0)            | [5 0]\n",
      "7    | v_7 = SIN(x_0, x_3)            | [0 3]\n",
      "8    | v_8 = COS(v_4, v_5)            | [ 9 10]\n",
      "9    | v_9 = SIN(x_0, x_0)            | [0 0]\n",
      "10   | v_10 = DIV(v_5, x_1)           | [10  1]\n",
      "11   | v_11 = DIV(v_5, x_3)           | [10  3]\n",
      "12   | v_12 = DIV(x_1, v_8)           | [ 1 13]\n",
      "13   | v_13 = COS(v_7, v_10)          | [12 15]\n",
      "14   | v_14 = SUB(v_6, x_3)           | [11  3]\n",
      "15   | v_15 = SUB(v_14, v_2)          | [19  7]\n",
      "16   | v_16 = DIV(v_6, v_7)           | [11 12]\n",
      "17   | v_17 = DIV(v_8, v_11)          | [13 16]\n",
      "18   | v_18 = ADD(v_6, x_4)           | [11  4]\n",
      "19   | v_19 = SIN(v_17, v_17)         | [22 22]\n"
     ]
    }
   ],
   "source": [
    "best_solution_index = jnp.argmax(evaluated_pop.fitness)\n",
    "best_genome = evaluated_pop[int(best_solution_index)]\n",
    "print(best_genome.render(config, OP_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33207d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import chex\n",
    "from typing import Any, TypeVar, Generic\n",
    "\n",
    "# Generic Types\n",
    "G = TypeVar(\"G\", bound=\"BaseGenome\")\n",
    "C = TypeVar(\"C\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. ABSTRACT BASE: MUTATION\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class BaseMutation(Generic[G, C]):\n",
    "    \"\"\"\n",
    "    Abstract Mutation Functor.\n",
    "    \"\"\"\n",
    "    # --- STATIC PARAMS (Re-compile if changed) ---\n",
    "    # Determines the output shape: (N_Offspring, ...)\n",
    "    num_offspring: int = struct.field(pytree_node=False, default=1)\n",
    "\n",
    "    def __call__(self, key: chex.PRNGKey, genome: G, config: C) -> G:\n",
    "        \"\"\"\n",
    "        Applies mutation to produce 'num_offspring' children.\n",
    "        Output Shape: (Num_Offspring, Genome_Size...)\n",
    "        \"\"\"\n",
    "        # 1. Split keys for the static number of children\n",
    "        keys = jax.random.split(key, self.num_offspring)\n",
    "        \n",
    "        # 2. Vectorize the single mutation logic\n",
    "        # vmap over keys (axis 0), keep genome & config constant\n",
    "        return jax.vmap(\n",
    "            lambda k, g, c: self._mutate_one(k, g, c), \n",
    "            in_axes=(0, None, None)\n",
    "        )(keys, genome, config)\n",
    "\n",
    "    def _mutate_one(self, key: chex.PRNGKey, genome: G, config: C) -> G:\n",
    "        \"\"\"Abstract: Logic to produce EXACTLY ONE mutant.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. ABSTRACT BASE: CROSSOVER\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class BaseCrossover(Generic[G, C]):\n",
    "    \"\"\"\n",
    "    Abstract Crossover Functor.\n",
    "    \"\"\"\n",
    "    # --- STATIC PARAMS (Re-compile if changed) ---\n",
    "    num_offspring: int = struct.field(pytree_node=False, default=1)\n",
    "\n",
    "    def __call__(self, key: chex.PRNGKey, p1: G, p2: G, config: C) -> G:\n",
    "        \"\"\"\n",
    "        Combines two parents to produce 'num_offspring' children.\n",
    "        Output Shape: (Num_Offspring, Genome_Size...)\n",
    "        \"\"\"\n",
    "        keys = jax.random.split(key, self.num_offspring)\n",
    "        \n",
    "        # vmap over keys (axis 0), keep parents & config constant\n",
    "        return jax.vmap(\n",
    "            lambda k, a, b, c: self._cross_one(k, a, b, c),\n",
    "            in_axes=(0, None, None, None)\n",
    "        )(keys, p1, p2, config)\n",
    "\n",
    "    def _cross_one(self, key: chex.PRNGKey, p1: G, p2: G, config: C) -> G:\n",
    "        \"\"\"Abstract: Logic to produce EXACTLY ONE child.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. CONCRETE: LINEAR MUTATION\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class LinearMutation(BaseMutation[LinearGenome, LinearGenomeConfig]):\n",
    "    \"\"\"\n",
    "    Standard Linear GP Mutation.\n",
    "    \"\"\"\n",
    "    # --- DYNAMIC PARAMS (Annealable/Mutable) ---\n",
    "    # These are treated as inputs to the kernel.\n",
    "    op_rate: float  = 0.1   # Prob to flip Opcode\n",
    "    arg_rate: float = 0.1   # Prob to flip Argument\n",
    "\n",
    "    def _mutate_one(self, key: chex.PRNGKey, genome: LinearGenome, config: LinearGenomeConfig) -> LinearGenome:\n",
    "        k_op, k_arg, k_noise = jax.random.split(key, 3)\n",
    "\n",
    "        # 1. Generate Boolean Masks (Where to mutate?)\n",
    "        # Uses self.op_rate / self.arg_rate dynamically\n",
    "        mask_ops = jax.random.bernoulli(k_op, self.op_rate, genome.ops.shape)\n",
    "        mask_args = jax.random.bernoulli(k_arg, self.arg_rate, genome.args.shape)\n",
    "\n",
    "        # 2. Generate Random Noise (New values)\n",
    "        # Note: We reuse random generation logic inline for speed\n",
    "        noise_ops = jax.random.randint(k_noise, genome.ops.shape, 0, config.num_ops)\n",
    "        \n",
    "        # Args can point up to (Inputs + Length)\n",
    "        # Autocorrect will fix forward-references later\n",
    "        max_mem = config.num_inputs + config.length\n",
    "        # Re-split k_noise for args\n",
    "        k_noise_args = jax.random.split(k_noise)[0]\n",
    "        noise_args = jax.random.randint(k_noise_args, genome.args.shape, 0, max_mem)\n",
    "\n",
    "        # 3. Apply Changes (If mask is True, take Noise, else take Old)\n",
    "        new_ops = jnp.where(mask_ops, noise_ops, genome.ops)\n",
    "        new_args = jnp.where(mask_args, noise_args, genome.args)\n",
    "\n",
    "        # 4. Construct & Repair\n",
    "        # Enforce Topological Validity (DAG)\n",
    "        return genome.replace(ops=new_ops, args=new_args).autocorrect(config)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. CONCRETE: LINEAR CROSSOVER\n",
    "# ==========================================\n",
    "@struct.dataclass\n",
    "class LinearCrossover(BaseCrossover[LinearGenome, LinearGenomeConfig]):\n",
    "    \"\"\"\n",
    "    Uniform Crossover (Coin Flip Mixing).\n",
    "    \"\"\"\n",
    "    # --- DYNAMIC PARAMS ---\n",
    "    mixing_ratio: float = 0.5  # 0.5 = Balanced mix\n",
    "\n",
    "    def _cross_one(self, key: chex.PRNGKey, p1: LinearGenome, p2: LinearGenome, config: LinearGenomeConfig) -> LinearGenome:\n",
    "        # 1. Generate Mixing Mask\n",
    "        # True = From P1, False = From P2\n",
    "        mask = jax.random.bernoulli(key, self.mixing_ratio, p1.ops.shape)\n",
    "\n",
    "        # 2. Mix Opcodes\n",
    "        child_ops = jnp.where(mask, p1.ops, p2.ops)\n",
    "\n",
    "        # 3. Mix Arguments\n",
    "        # Broadcast mask (L,) -> (L, Arity) to keep args coupled\n",
    "        mask_expanded = mask[:, None]\n",
    "        child_args = jnp.where(mask_expanded, p1.args, p2.args)\n",
    "\n",
    "        # 4. Return\n",
    "        # Uniform crossover between two valid topological parents \n",
    "        # is guaranteed to be valid, so no autocorrect needed.\n",
    "        return p1.replace(ops=child_ops, args=child_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c7feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Output Shape: (1, 10)\n",
      "Explosive Output Shape: (10, 10)\n",
      "Twins Shape: (2, 10)\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup\n",
    "config = LinearGenomeConfig(length=10, num_inputs=2, num_ops=5)\n",
    "key = jax.random.PRNGKey(42)\n",
    "parent = LinearGenome.random_init(key, config)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# SCENARIO A: Standard Evolution (1 Parent -> 1 Child)\n",
    "# -----------------------------------------------------------\n",
    "mut_standard = LinearMutation(num_offspring=1, op_rate=0.1, arg_rate=0.1)\n",
    "child_batch = mut_standard(key, parent, config)\n",
    "\n",
    "print(f\"Standard Output Shape: {child_batch.ops.shape}\")\n",
    "# Output: (1, 10) -> Batch of size 1\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# SCENARIO B: \"Explosive\" Evolution (1 Parent -> 10 Mutants)\n",
    "# (Useful for Evolution Strategies / ES)\n",
    "# -----------------------------------------------------------\n",
    "mut_explosive = LinearMutation(num_offspring=10, op_rate=0.2, arg_rate=0.2)\n",
    "cloud_batch = mut_explosive(key, parent, config)\n",
    "\n",
    "print(f\"Explosive Output Shape: {cloud_batch.ops.shape}\")\n",
    "# Output: (10, 10) -> Batch of size 10 generated instantly\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# SCENARIO C: Twin Crossover (2 Parents -> 2 Children)\n",
    "# -----------------------------------------------------------\n",
    "cross_twin = LinearCrossover(num_offspring=2, mixing_ratio=0.5)\n",
    "p1 = LinearGenome.random_init(key, config)\n",
    "p2 = LinearGenome.random_init(key, config)\n",
    "\n",
    "twins = cross_twin(key, p1, p2, config)\n",
    "print(f\"Twins Shape: {twins.ops.shape}\")\n",
    "# Output: (2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70950e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MalthusJax Variation Demo ===\n",
      "\n",
      "--- Parent 1 ---\n",
      "Row  | Expression                     | Raw\n",
      "--------------------------------------------------\n",
      "0    | v_0 = SIN(x_0, x_1)            | [0 1]\n",
      "1    | v_1 = SUB(x_0, x_0)            | [0 0]\n",
      "2    | v_2 = SUB(v_1, v_1)            | [3 3]\n",
      "3    | v_3 = SIN(v_2, v_0)            | [4 2]\n",
      "4    | v_4 = SIN(x_1, x_1)            | [1 1]\n",
      "\n",
      "\n",
      ">>> Applying Twin Crossover (2 Offspring)...\n",
      "Output Shape: (2, 5)\n",
      "\n",
      "--- Child 0 (Mixed Logic) ---\n",
      "Row  | Expression                     | Raw\n",
      "--------------------------------------------------\n",
      "0    | v_0 = SIN(x_0, x_1)            | [0 1]\n",
      "1    | v_1 = MUL(x_0, x_1)            | [0 1]\n",
      "2    | v_2 = SUB(v_1, v_1)            | [3 3]\n",
      "3    | v_3 = SIN(v_2, v_0)            | [4 2]\n",
      "4    | v_4 = MUL(v_1, v_2)            | [3 4]\n",
      "\n",
      "\n",
      ">>> Applying Explosive Mutation (3 Mutants, High Rate)...\n",
      "Output Shape: (3, 5)\n",
      "\n",
      "--- Mutant 0 (Noisy & Autocorrected) ---\n",
      "Row  | Expression                     | Raw\n",
      "--------------------------------------------------\n",
      "0    | v_0 = SIN(x_1, x_1)            | [1 1]\n",
      "1    | v_1 = SUB(x_1, v_0)            | [1 2]\n",
      "2    | v_2 = SIN(v_1, v_1)            | [3 3]\n",
      "3    | v_3 = SIN(v_2, v_0)            | [4 2]\n",
      "4    | v_4 = SIN(x_1, x_1)            | [1 1]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import chex\n",
    "\n",
    "\n",
    "def run_variation_demo():\n",
    "    print(\"=== MalthusJax Variation Demo ===\\n\")\n",
    "    \n",
    "    # 1. SETUP\n",
    "    # Length 5, 2 Inputs (x0, x1), 4 Ops (ADD, SUB, MUL, SIN)\n",
    "    config = LinearGenomeConfig(length=5, num_inputs=2, num_ops=4)\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    op_names = [\"ADD\", \"SUB\", \"MUL\", \"SIN\"]\n",
    "\n",
    "    # 2. CREATE PARENTS\n",
    "    k1, k2, k3 = jax.random.split(key, 3)\n",
    "    p1 = LinearGenome.random_init(k1, config)\n",
    "    p2 = LinearGenome.random_init(k2, config)\n",
    "\n",
    "    print(\"--- Parent 1 ---\")\n",
    "    print(p1.render(config, op_names))\n",
    "    \n",
    "    # 3. TEST CROSSOVER (Static: 2 Children, Dynamic: 50% Mix)\n",
    "    print(\"\\n\\n>>> Applying Twin Crossover (2 Offspring)...\")\n",
    "    crossover_op = LinearCrossover(num_offspring=2, mixing_ratio=0.5)\n",
    "    \n",
    "    # Run JIT-compiled\n",
    "    @jax.jit\n",
    "    def run_cross(k, a, b):\n",
    "        return crossover_op(k, a, b, config)\n",
    "\n",
    "    twins = run_cross(k3, p1, p2)\n",
    "    \n",
    "    print(f\"Output Shape: {twins.ops.shape}\") # Should be (2, 5)\n",
    "    \n",
    "    # Slice the batch to see Child 0\n",
    "    child_0 = jax.tree_util.tree_map(lambda x: x[0], twins)\n",
    "    print(\"\\n--- Child 0 (Mixed Logic) ---\")\n",
    "    print(child_0.render(config, op_names))\n",
    "\n",
    "    # 4. TEST MUTATION (Static: 3 Mutants, Dynamic: High Noise)\n",
    "    print(\"\\n\\n>>> Applying Explosive Mutation (3 Mutants, High Rate)...\")\n",
    "    # High mutation rate to force changes\n",
    "    mutation_op = LinearMutation(num_offspring=3, op_rate=0.5, arg_rate=0.5)\n",
    "    \n",
    "    @jax.jit\n",
    "    def run_mut(k, g):\n",
    "        return mutation_op(k, g, config)\n",
    "\n",
    "    k_mut = jax.random.split(key)[0]\n",
    "    mutants = run_mut(k_mut, p1)\n",
    "    \n",
    "    print(f\"Output Shape: {mutants.ops.shape}\") # Should be (3, 5)\n",
    "    \n",
    "    # Slice to see Mutant 0\n",
    "    mutant_0 = jax.tree_util.tree_map(lambda x: x[0], mutants)\n",
    "    print(\"\\n--- Mutant 0 (Noisy & Autocorrected) ---\")\n",
    "    print(mutant_0.render(config, op_names))\n",
    "\n",
    "# Run it\n",
    "run_variation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d82b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JIT Compilation Verification ===\n",
      "\n",
      "1. Compiling Initialization... Done.\n",
      "   Parent 1 Shape: (5,)\n",
      "   > Row 0: Row  | Expression                     | Raw\n",
      "\n",
      "2. Compiling Twin Crossover (Static=2 Children)... Done.\n",
      "   Offspring Batch Shape: (2, 5) (Should be 2, 5)\n",
      "   > Child 0 Row 0: Row  | Expression                     | Raw\n",
      "\n",
      "3. Compiling Explosive Mutation (Static=5 Mutants)... Done.\n",
      "   Mutant Batch Shape: (5, 5) (Should be 5, 5)\n",
      "   > Mutant 0 Row 0: Row  | Expression                     | Raw\n",
      "\n",
      "[SUCCESS] All operators compiled and ran on accelerator.\n"
     ]
    }
   ],
   "source": [
    "def run_jit_demo():\n",
    "    print(\"=== JIT Compilation Verification ===\\n\")\n",
    "    config = LinearGenomeConfig(length=5, num_inputs=2, num_ops=10)\n",
    "    master_key = jax.random.PRNGKey(42)\n",
    "\n",
    "    # --- STEP A: JIT-Compiled Initialization ---\n",
    "    print(\"1. Compiling Initialization...\", end=\" \")\n",
    "    \n",
    "    @jax.jit\n",
    "    def create_parent(key):\n",
    "        return LinearGenome.random_init(key, config)\n",
    "    \n",
    "    # Run once to compile\n",
    "    k1, k2, k3 = jax.random.split(master_key, 3)\n",
    "    parent_1 = create_parent(k1)\n",
    "    parent_2 = create_parent(k2)\n",
    "    print(\"Done.\")\n",
    "    print(f\"   Parent 1 Shape: {parent_1.ops.shape}\")\n",
    "    print(f\"   > Row 0: {parent_1.render(config).splitlines()[0]}\")\n",
    "\n",
    "    # --- STEP B: JIT-Compiled Crossover (Twins) ---\n",
    "    print(\"\\n2. Compiling Twin Crossover (Static=2 Children)...\", end=\" \")\n",
    "    \n",
    "    # Instantiate operator OUTSIDE JIT (Configures static shapes)\n",
    "    crossover_op = LinearCrossover(num_offspring=2, mixing_ratio=0.5)\n",
    "\n",
    "    @jax.jit\n",
    "    def run_crossover(key, p1, p2):\n",
    "        return crossover_op(key, p1, p2, config)\n",
    "\n",
    "    # Run execution\n",
    "    offspring_batch = run_crossover(k3, parent_1, parent_2)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    # Check Result\n",
    "    print(f\"   Offspring Batch Shape: {offspring_batch.ops.shape} (Should be 2, 5)\")\n",
    "    # Access Child 0 from batch\n",
    "    child_0 = jax.tree_util.tree_map(lambda x: x[0], offspring_batch)\n",
    "    print(f\"   > Child 0 Row 0: {child_0.render(config).splitlines()[0]}\")\n",
    "\n",
    "    # --- STEP C: JIT-Compiled Mutation (Explosive) ---\n",
    "    print(\"\\n3. Compiling Explosive Mutation (Static=5 Mutants)...\", end=\" \")\n",
    "    \n",
    "    # Instantiate operator with different Static Config\n",
    "    mutation_op = LinearMutation(num_offspring=5, op_rate=0.2, arg_rate=0.2)\n",
    "\n",
    "    @jax.jit\n",
    "    def run_mutation(key, genome):\n",
    "        return mutation_op(key, genome, config)\n",
    "\n",
    "    k_mut = jax.random.split(master_key)[0]\n",
    "    mutant_batch = run_mutation(k_mut, parent_1)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    # Check Result\n",
    "    print(f\"   Mutant Batch Shape: {mutant_batch.ops.shape} (Should be 5, 5)\")\n",
    "    \n",
    "    # Verify Autocorrect worked (Row 0 must imply inputs only)\n",
    "    mutant_0 = jax.tree_util.tree_map(lambda x: x[0], mutant_batch)\n",
    "    print(f\"   > Mutant 0 Row 0: {mutant_0.render(config).splitlines()[0]}\")\n",
    "    \n",
    "    print(\"\\n[SUCCESS] All operators compiled and ran on accelerator.\")\n",
    "\n",
    "run_jit_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e156154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import chex\n",
    "from typing import Generic\n",
    "\n",
    "# Generic Genome Type not needed here, strictly math!\n",
    "\n",
    "@struct.dataclass\n",
    "class BaseSelection:\n",
    "    \"\"\"\n",
    "    Abstract Selection Operator.\n",
    "    Operates purely on Fitness Arrays to return Indices.\n",
    "    \"\"\"\n",
    "    # STATIC: How many parents do we want to pick?\n",
    "    num_selections: int = struct.field(pytree_node=False)\n",
    "\n",
    "    def __call__(self, key: chex.PRNGKey, fitness: chex.Array) -> chex.Array:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            key: RNG Key\n",
    "            fitness: Shape (Pop_Size, ...) - Can be 1D or 2D (Symbiotic)\n",
    "            \n",
    "        Returns:\n",
    "            Selected Indices: Shape (num_selections,) int32\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbdc2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class SymbioticTournament(BaseSelection):\n",
    "    # STATIC CONFIG\n",
    "    tournament_size: int = struct.field(pytree_node=False, default=3)\n",
    "    symbionts_per_genome: int = struct.field(pytree_node=False, default=3)\n",
    "\n",
    "    def __call__(self, key: chex.PRNGKey, fitness_matrix: chex.Array) -> chex.Array:\n",
    "        \"\"\"\n",
    "        Performs tournament selection on the 'unrolled' best atomic trees.\n",
    "        \n",
    "        Args:\n",
    "            fitness_matrix: Shape (N, L) - Fitness of every atomic tree.\n",
    "            \n",
    "        Returns:\n",
    "            indices: Shape (num_selections,) - Indices of the winning GENOMES (0..N-1).\n",
    "        \"\"\"\n",
    "        N, L = fitness_matrix.shape\n",
    "        \n",
    "        # 1. Filter: Get the Top K Symbionts per Genome\n",
    "        # We don't want all 50 instructions entering (too much garbage).\n",
    "        # We pick the elite sub-components.\n",
    "        # values shape: (N, K)\n",
    "        top_k_values, _ = jax.lax.top_k(fitness_matrix, self.symbionts_per_genome)\n",
    "        \n",
    "        # 2. Flatten: Create the \"Symbiont Pool\"\n",
    "        # We treat this as a population of size (N * K)\n",
    "        pool_fitness = top_k_values.ravel() # Shape (N*K, )\n",
    "        total_candidates = pool_fitness.shape[0]\n",
    "        \n",
    "        # 3. Tournament: Select Winning Symbionts\n",
    "        \n",
    "        # A. Pick Random Contenders (Indices into the pool)\n",
    "        # Shape: (Num_Selections, Tournament_Size)\n",
    "        k_tourn = jax.random.split(key)[0]\n",
    "        contestants = jax.random.randint(\n",
    "            k_tourn, \n",
    "            shape=(self.num_selections, self.tournament_size), \n",
    "            minval=0, \n",
    "            maxval=total_candidates\n",
    "        )\n",
    "        \n",
    "        # B. Get their scores\n",
    "        # We use 'take' to gather fitness values\n",
    "        scores = jnp.take(pool_fitness, contestants)\n",
    "        \n",
    "        # C. Find the Winner (Argmax)\n",
    "        # Shape: (Num_Selections,) -> Local index 0..T-1\n",
    "        winner_local_idx = jnp.argmax(scores, axis=1)\n",
    "        \n",
    "        # D. Get the Winner's Pool Index\n",
    "        # advanced indexing: pick the winner from the contestants row\n",
    "        # vmap over the rows\n",
    "        winner_pool_indices = jax.vmap(lambda row, idx: row[idx])(contestants, winner_local_idx)\n",
    "        \n",
    "        # 4. Map Back: Symbiont ID -> Genome ID\n",
    "        # If we have 3 symbionts per genome:\n",
    "        # Symbiont 0, 1, 2  -> Genome 0\n",
    "        # Symbiont 3, 4, 5  -> Genome 1\n",
    "        # Formula: Genome_ID = Symbiont_ID // K\n",
    "        \n",
    "        winner_genome_indices = winner_pool_indices // self.symbionts_per_genome\n",
    "        \n",
    "        return winner_genome_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9225829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop.fitness.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2936333",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m selector = SymbioticTournament(\n\u001b[32m      4\u001b[39m     num_selections=\u001b[32m100\u001b[39m,    \u001b[38;5;66;03m# We want to select 100 parents\u001b[39;00m\n\u001b[32m      5\u001b[39m     tournament_size=\u001b[32m4\u001b[39m,     \u001b[38;5;66;03m# Standard pressure\u001b[39;00m\n\u001b[32m      6\u001b[39m     symbionts_per_genome=\u001b[32m5\u001b[39m \u001b[38;5;66;03m# Let the top 5 atomic trees represent the genome\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. THE SELECTION STEP (Pure Math)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# \"Here are the scores, who wins?\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Returns: Array([0, 0, 5, 12, 99...])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m selected_indices = \u001b[43mselector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfitness\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 3. THE GATHERING STEP (Data Movement)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# \"Okay, grab those genomes.\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Your BasePopulation.__getitem__ handles this automatically!\u001b[39;00m\n\u001b[32m     17\u001b[39m parents = pop[selected_indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mSymbioticTournament.__call__\u001b[39m\u001b[34m(self, key, fitness_matrix)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: chex.PRNGKey, fitness_matrix: chex.Array) -> chex.Array:\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    Performs tournament selection on the 'unrolled' best atomic trees.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33;03m        indices: Shape (num_selections,) - Indices of the winning GENOMES (0..N-1).\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     N, L = fitness_matrix.shape\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# 1. Filter: Get the Top K Symbionts per Genome\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# We don't want all 50 instructions entering (too much garbage).\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# We pick the elite sub-components.\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# values shape: (N, K)\u001b[39;00m\n\u001b[32m     23\u001b[39m     top_k_values, _ = jax.lax.top_k(fitness_matrix, \u001b[38;5;28mself\u001b[39m.symbionts_per_genome)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# 1. Setup\n",
    "# Assume pop.fitness is (100, 50) -> 100 Genomes, 50 Atomic Trees each\n",
    "selector = SymbioticTournament(\n",
    "    num_selections=100,    # We want to select 100 parents\n",
    "    tournament_size=4,     # Standard pressure\n",
    "    symbionts_per_genome=5 # Let the top 5 atomic trees represent the genome\n",
    ")\n",
    "\n",
    "# 2. THE SELECTION STEP (Pure Math)\n",
    "# \"Here are the scores, who wins?\"\n",
    "# Returns: Array([0, 0, 5, 12, 99...])\n",
    "selected_indices = selector(key, pop.fitness)\n",
    "\n",
    "# 3. THE GATHERING STEP (Data Movement)\n",
    "# \"Okay, grab those genomes.\"\n",
    "# Your BasePopulation.__getitem__ handles this automatically!\n",
    "parents = pop[selected_indices]\n",
    "\n",
    "# Now 'parents' is a LinearPopulation of size 100, ready for Crossover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9e0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
